# Parameters to setup experiment.
experiment:
  # Experiment logs will be stored at "log_path"
  log_path:
  cuda: cuda:0
  # Seed for random number generators (for repeatability).
  randomseed:
  # Number of training iterations.
  start_epoch: 1
  end_epoch: 2000
  # Number of rays to use per iteration, i.e. batch size
  batch_size: 8 * 1  # 173163 *
  # Number of training iterations after which to checkpoint.
  save_every_epoch: 500
  # Number of training iterations aftger which to print progress.
  eval_every_iter: 50

# Dataset parameters.
dataset:
  # Base directory of dataset.
  data_path:
  gray_scale: False
  shadow_threshold: 0.1

  # none for all inputs.
  sparse_input_random_seed:
  sparse_input:

# Model parameters.
models:
  load_checkpoint: False
  checkpoint_path:

  # nerf model.
  nerf:
    num_layers: 8
    hidden_size: 256
    skip_connect_every: 3
    num_encoding_fn_input1: 10
    num_encoding_fn_input2: 0
    include_input_input1: 2   # denote images coordinates (ix, iy)
    include_input_input2: 0   # denote lighting direcions (lx, ly, lz), or half-angle (theta_h, theta_d)

  use_onlyspecular: False
  use_onlydiffuse: False

  specular:
    type: Spherical_Gaussian
    num_basis: 12  # number of basis to output,  if use RGB, output_ch*3
    k_low: 10
    k_high: 300
    trainable_k: True
    dynamic_basis: True

  light_model:
    type: Light_Model_CNN   # options: {MLP,  Light_Model_CNN,  Latent,  None, LNet}
    explicit_direction: True
    explicit_intensity: True
    num_layers: 3
    hidden_size: 64
    skip_connect_every: 8
    # for {Light_Model_CNN, LNet}
    load_pretrain: runs/pre_trained_LNet/model_params.pth
    # if type: None, add noise to init lights.
    # noise<0 means gt init, noise=0 means 0 init, noise>0 add gaussian noise
    ld_noise: 0
    li_noise: 0
  use_mean_var: True


# indexer params.
loss:
  # Name of loss function
  rgb_loss: l1   # options are 'l1', 'l2', 'smoothl1', 'MaxConstrain_L1Loss'
  spec_coeff_factor: 0.0E-3
  diff_tv_factor: 1.0E-2
  spec_tv_factor: 1.0E-2
  normal_tv_factor: 1.0E-2
  contour_factor: 1.0E-1  # for constrain normals at contour to be 90degrees
  regularize_epoches: 0.25

# Optimizer params.
optimizer:
  # Name of the torch.optim class used for optimization.
  type: Adam
  # Learning rate.
  lr: 1.0E-3

# scheduler params.
scheduler:
  # Change is learning rate Per ? epoch
  step_size: 500000
  # rate that learning rate degrade, 1 is not changing
  gamma: 1
